prometheus:
  prometheusSpec:
    # resources:
    #   limits:
    #     cpu: 1
    #     memory: 1Gi
    #   requests:
    #     cpu: 100m
    #     memory: 100Mi
    configMaps:
      - bosh-target-groups
    replicas: 1
    externalUrl: ((prometheus_url))
    externalLabels:
      cluster: ((cluster_name))
      foundation: ((foundation))
    secrets:
      - etcd-client
    ruleSelector:
      matchLabels:
        app: prometheus-operator
        release: prometheus-operator
    portName: http-service
    enableAdminAPI: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: thin-disk
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    additionalScrapeConfigs:
    - job_name: "prometheus-cluster"
      metrics_path: "/federate"
      honor_labels: true
      scrape_interval: 30s
      scheme: http
      params:
        'match[]':
          - '{job="apiserver"}'
      static_configs:
      - targets: ((federated_targets))
    - job_name: "kubeEtcd-((cluster_name))"
      file_sd_configs:
      - files:
        - '/etc/prometheus/configmaps/bosh-target-groups/bosh_target_groups.json'
      scheme: https
      tls_config:
        ca_file: "/etc/prometheus/secrets/etcd-client/etcd-client-ca.crt"
        cert_file: "/etc/prometheus/secrets/etcd-client/etcd-client.crt"
        key_file: "/etc/prometheus/secrets/etcd-client/etcd-client.key"
        server_name: "*.etcd.cfcr.internal"
      relabel_configs:
      - source_labels: [__meta_bosh_job_process_name]
        regex: etcd
        action: keep
      - source_labels: [__meta_bosh_deployment]
        regex: ((service_instance_id))
        action: keep
      - source_labels: [__address__]
        regex: '(.*)'
        target_label: __address__
        replacement: '${1}:2379'

additionalPrometheusRulesMap:
  alert-rules:
    groups:
    - name: pks_rules
      rules:
      - alert: PksApiDown
        annotations:
          summary: 'PKS api is down'
          description: 'PKS api is unreachable in the last 5 minutes on {{ .ExternalLabels.foundation }}.'
        expr: wf_opp_pks_api_up < 1
        for: 5m
        labels:
          severity: critical
          group: pks_rules
    - name: istio_rules
      rules:
      - alert: IstioPilotAvailabilityDrop
        annotations:
          summary: 'Istio Pilot Availability Drop'
          description: 'Pilot pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
        expr: >
              avg(avg_over_time(up{job="pilot"}[1m])) < 0.5
        for: 5m
        labels:
          severity: critical
          group: istio_rules
      - alert: IstioGatewayAvailabilityDrop
        annotations:
          summary: 'Istio Gateway Availability Drop'
          description: 'Gateway pods have dropped during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Inbound traffic will likely be affected'
        expr: >
          min(kube_deployment_status_replicas_available{deployment="istio-ingressgateway", namespace="istio-system"}) without (instance, pod) < 2
        for: 5m
        labels:
          severity: critical
          group: istio_rules
      - alert: IstioPilotPushErrorsHigh
        annotations:
          summary: 'Number of Istio Pilot push errors is too high'
          description: 'Pilot has too many push errors during the last 5m (current value: *{{ printf "%2.0f%%" $value }}*). Envoy sidecars might have outdated configuration'
        expr: >
          sum(irate(pilot_xds_push_errors{job="pilot"}[5m])) / sum(irate(pilot_xds_pushes{job="pilot"}[5m])) > 0.05
        for: 5m
        labels:
          severity: critical
          group: istio_rules
      - alert: IstioGlobalHTTP5xxRateHigh
        annotations:
          summary: 'Istio Percentage of HTTP 5xx responses is too high'
          description: 'Istio global HTTP 5xx rate is too high in last 5m (current value: *{{ printf "%2.0f%%" $value }}*). The HTTP 5xx errors within the service mesh is unusually high'
        expr: >
          sum(irate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) / sum(irate(istio_requests_total{reporter="destination"}[5m])) > 0.01
        for: 5m
        labels:
          severity: critical
          group: istio_rules

  recording-rules:
    groups:
    - name: kube_api_rules
      interval: 15s
      rules:
      - record: "kubernetes:job_verb:apiserver_latency:pctl90rate5m"
        expr: "histogram_quantile ( 0.90, sum by (le, foundation, cluster, job, verb)( rate(apiserver_request_latencies_bucket[5m]) ) ) / 1e3 > 0"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes:job_verb_instance:apiserver_latency:pctl90rate5m"
        expr: "histogram_quantile ( 0.90, sum by (le, foundation, cluster, job, verb, instance)( rate(apiserver_request_latencies_bucket[5m]) ) ) / 1e3"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job:probe_success"
        expr: "sum by()(probe_success{provider=\"kubernetes\", component=\"apiserver\"})"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job_verb_code:apiserver_requests:rate5m"
        expr: "sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes:job_verb_code_instance:apiserver_requests:rate5m"
        expr: "sum by (foundation, cluster, job, verb, code, instance)(rate(apiserver_request_count[5m]))"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job_verb_code:apiserver_requests:ratio_rate5m"
        expr: "sum without (instance)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m)"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m"
        expr: "kubernetes:job_verb_code_instance:apiserver_requests:rate5m / ignoring(verb, code) group_left sum by (foundation, cluster, job, instance)(kubernetes:job_verb_code_instance:apiserver_requests:rate5m)"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes:job:apiserver_request_errors:ratio_rate5m"
        expr: "sum by (foundation, cluster, job)(kubernetes:job_verb_code_instance:apiserver_requests:ratio_rate5m{verb=~\"GET|POST|DELETE|PATCH\", code=~\"5..\"})"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job:apiserver_latency:pctl90rate5m"
        expr: "histogram_quantile ( 0.90, sum by (le, foundation, cluster, job)( rate(apiserver_request_latencies_bucket{verb=~\"GET|POST|DELETE|PATCH\"}[5m]) ) ) / 1e3"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job:slo_kube_api_ok"
        expr: "kubernetes:job:apiserver_request_errors:ratio_rate5m < bool 0.01 * kubernetes::job:apiserver_latency:pctl90rate5m < bool 200"
        labels:
          job: "kubernetes_api_slo"
      - record: "kubernetes::job:slo_kube_api_sample"
        expr: "kubernetes:job:apiserver_request_errors:ratio_rate5m < bool Inf * kubernetes::job:apiserver_latency:pctl90rate5m < bool Inf"
        labels:
          job: "kubernetes_api_slo"
